# egoallo

**[Project page](https://egoallo.github.io/) &bull;
[arXiv](https://arxiv.org/abs/2410.03665)**

Code release for our preprint:

<table><tr><td>
    Brent Yi<sup>1</sup>, Vickie Ye<sup>1</sup>, Maya Zheng<sup>1</sup>, Lea M&uuml;ller<sup>1</sup>, Georgios Pavlakos<sup>2</sup>, Yi Ma<sup>1</sup>, Jitendra Malik<sup>1</sup>, and Angjoo Kanazawa<sup>1</sup>.
    <strong>Estimating Body and Hand Motion in an Ego-sensed World.</strong>
    arXiV, 2024.
</td></tr>
</table>
<sup>1</sup><em>UC Berkeley</em>, <sup>2</sup><em>UT Austin</em>

---

**TLDR;** We use egocentric SLAM poses and images to estimate 3D human body pose, height, and hands.

https://github.com/user-attachments/assets/7d28e07f-ab83-4749-ac6b-abe692d9ba20


---

**Updates**

- _Oct 7, 2024:_ Initial release.

**Status**

This repository currently contains:

- [x] `egoallo` package, which contains reference training and sampling implementation details.
- [x] Training script.

We are preparing and will release by October 14:

- [ ] Setup instructions.
- [ ] Model checkpoints.
- [ ] Dataset preprocessing script.
- [ ] Inference script.
- [ ] Visualization script.
